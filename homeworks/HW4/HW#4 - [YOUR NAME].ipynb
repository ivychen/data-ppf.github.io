{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4  \n",
    "# \"People like you...\" (part 1 of 2)\n",
    "### Assigned Monday, 23 Apr 2018  /  Due Friday, 27 Apr  2018 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal:\n",
    "\n",
    "In lab 11 we used cosine simularity to compare users to users and movies to movies. Here we're going to use cosine simularity and PCA to postulate relationships between individuals. We're then going to reflect on the implications of what we did. <b>This homework should take between 1 and 2 hours to do, but no more than that.</b>   \n",
    "\n",
    "More specifically, you're going to\n",
    "1. Get the \"Adult\" machine learning data set and convert nonnumerical information into numerical values;\n",
    "2. Employ cosine simularities to compare individuals;\n",
    "3. Use PCA to graphically represent this data in two dimensions;\n",
    "4. Use K-means on PCA results to generate clusters within data (in homework 5).\n",
    "5. Reflect upon the ethical and epistemological implications of this approach and the construction of the data set (in homework 5). \n",
    "\n",
    "We'll build on what we do here in homework 5, where we'll take our PCA results and perform K-means on them to generate clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Problems \n",
    "####  Be sure to rename this homework so that it includes your name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 1 [20 points]\n",
    "1. Download the \"adult\" data set <b>we used in Lab 12b (\"FAT\")</b> from the machine learning repository (https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data) and import this into a pandas dataframe named \"data\". I've given you the column names in the list named \"features\" below. \n",
    "2. To get a sense of the data, use the `head` command.\n",
    "3. <b>Using the code in Lab 12b</b>, convert strings values in this data set to numerical values. \n",
    "4. Use `head` again to confirm you successfully converted features into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age','workclass', 'fnlwgt', 'education', 'education.num', 'marital.status','occupation', 'relationship','race','sex','capital.gain','capital.loss', 'hours.per.week', 'native.country','income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 2 [10 points]\n",
    "Now that we've converted all the categorical data to numerical data, let's normalize all of these variables (i.e., columns) to be between 0.0 and 1.0. This is an interpretative decision: we are effectively deciding not to allow any of these variables to have a greater maximum or minimum magnitude than any other variable. (We might also want to argue about how we coverted our categorical variables into continous variables.) Yet for the purposes of this homework, we press on.  \n",
    "\n",
    "We can take advantage of pandas to normalize our dataframe 'data'. In the interests of saving you time, here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize dataframe \"data\"\n",
    "normalized_data = (data-data.min())/(data.max()-data.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, much like what we did in lab 11 (\"database & recomendation) with films, we want to compute the cosine simularity between each person based on their attributes. Below I give you an example for producing cosine simularity scores for the first person in \"data\" and everyone else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this step gives you a memory error or kernel crash, use normalized_data[0:5000] in place of normalized_data\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_for_one_person = cosine_similarity(normalized_data.iloc[[0]], normalized_data)\n",
    "similarity_for_one_person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the above example, calculate the cosine simularity for the entire dataframe \"data\". Name this array `simularity`. \n",
    "\n",
    "Hint 1: The solution to this is just one line of code. To take cosine simularity of a whole array (rather than an individual row as we did above), you can enter the array as the sole argument of the consine_similarity function. You can check your answer by comparing the first row of `similarity` to the answer above for one person--they should match.  \n",
    "Hint 2: If the step below gives you a memory error or kernel crash, use `normalized_data[0:5000]` in place of `normalized_data`. If you still have problems, try smaller numbers until you can successfully calculate a subset of `similarity`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If this step gives you a memory error or kernel crash, use only normalized_data[0:5000] in place of normalized_data\n",
    "\n",
    "similarity = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 3 [25 points] \n",
    "Now we have a similarity matrix describing a set of relationships postulated between different individuals in the data set based on the similarity of the 15 features in the data set. We'd like to visualize this simularity to get some sense of how the individuals are distributed with respect to each other. To do this, we again return to our trusty unsupervised learning technique: PCA. \n",
    "\n",
    "For this question, make PCA plots for four different sets of people from `similarity`: \n",
    "1. PCA on the first 100 individuals in `similarity`. \n",
    "2. PCA on the first 500 individuals in `similarity`. \n",
    "3. PCA on the first 1500 individuals in `similarity`. \n",
    "4. PCA on the last 1500 individuals in `similarity`.\n",
    "\n",
    "[Note: If you have memory or kernal crash problems, use smaller numbers, e.g. 600 instead of 1500, etc.]\n",
    "\n",
    "We'll reflect deeply on the ethics of using this data set and on performing simularity measures on *people* in our next and final homework, using the work we did here as our starting point. For purposes of time we'll end this homework here with the caveat that there remains much to discuss and that we need to continue this conversation in part II of this homework (i.e., homework 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries to get you started\n",
    "from sklearn.decomposition import PCA as PCA \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your PCA code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
