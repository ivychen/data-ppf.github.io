{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Elementary Statistics, Exploratory Data Analysis, & Statistical Entities in Political Discourse\n",
    "\n",
    "In this lab you will \n",
    "1. learn how to perform some elementary statistics in python\n",
    "2. and use the python library called \"panadas\" to examine and explore a data set.\n",
    "\n",
    "Building off our earlier discussions of data provenance last week and our discussion on Tuesday regarding the relationship between our tools and the articulation of criteria for state formation and individual identity (pace Igo and Desrosieres), we will continue to be mindful of how our tools shape our expectations.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Open a webbrowser and type the following address: http://data-ppf.dsi.columbia.edu:8000\n",
    ". Log in to the Jupyterhub server with your user name and password, open a new notebook, and download this notebook using the wget command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget data-ppf.github.io/labs/Lab2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was mentioned last lab, you can use any unix command in a jupyter notebook by starting the line with an exclaimation point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Few Basic Tools for Examining Data\n",
    "Python provides a number of useful, powerful, and versatile tools for interrogating data. We're going to show you how to use a few of them. But first, to use these tools, we need to tell python to load the code libraries that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# loading libraries\n",
    "import matplotlib.pyplot as plt  #this is used to make pretty graphs\n",
    "import numpy as np               #this is used to calculate stuff quickly\n",
    "import ipywidgets                #this is used to make graphs interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any line that begins with a hash (\"#\") will not be executed in the code block. The \"as plt\" and \"as np\" allow us to refer to these libraries in the code as \"plt\" and \"np\" rather than \"plot\" and \"numpy,\" respectively. Let's begin by considering what may be the most famous curve in all of statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code to generate a normal distribution \"analytically\"\n",
    "\n",
    "# normal distribution parameters\n",
    "mu = 0 \n",
    "variance = 1 \n",
    "sigma = np.sqrt(variance)\n",
    "\n",
    "# plot normal distribution\n",
    "x = np.linspace(-4, 4, 100)  #sets range of x-axis\n",
    "plt.plot(x, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mu)**2 / (2 * sigma**2) ), linewidth=2, color='r') #determines what will be plotted \n",
    "plt.show() #display plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Class Discussion: what's going on in the code above?)\n",
    "\n",
    "This curve has many names, including the \"normal distribution,\" the \"normal curve,\" the \"normal correlation,\" the \"bell curve,\" the \"gaussian distribution,\" the \"law of deviation,\" the \"normal universe,\" the \"normal population,\"  or, today, often simply the \"gaussian\". It has lots of interesting [mathematical properties](https://en.wikipedia.org/wiki/Normal_distribution), but for our purposes you are free to think of it as one of many useful mathematical objects that tend to crop up frequently in a variety of different contexts.    \n",
    "\n",
    "Historical Aside: The gaussian distribution takes its name from [Carl Friedrich Gauss](https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss), however, this particular convention is yet another example of [Stigler's Law](https://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy) stating that a discovery is never named after the first person to discover it.[1] The name \"normal distribution\" was made popular by Karl Pearson--who also coined the term \"standard deviation\"--as a means of dispensing with concerns of discovery \"priority,\" but had the regrettable effect, he noted, \"of leading people to believe that all other distributions of frequency are in one sense or another 'abnormal.'\"[2] As early as 1733 Abraham De Moivre used this curve, but did not think of the curve as \"a probability density function.\"[3]   \n",
    "\n",
    "\n",
    "<hr>\n",
    "<small>\n",
    "1. Stephen Stigler, \"Stigler's Law of Eponymy,\" <em>Transactions of the New York Academy of Sciences</em> 39, issue 1, series II (1980): 147-157. See also Stigler, <em>Statistics on the Table</em>, Cambridge, MA: Harvard University Press, 1999, 277-290.\n",
    "\n",
    "2. Theodore Porter, <em>Karl Pearson: The scientific life in a statistical age</em>, Princeton: Princeton University Press, 2004, 237. Karl Pearson, \"Notes on the History of Correlation,\" <em>Biometrika</em> 13, no 1, (1920): 25.\n",
    "\n",
    "3. Stephen Stigler, <em>The History of Statistics</em>, Cambridge, MA: Harvard University Press, 1986, 76.  \n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you're rarely (if ever!) see a perfect gaussian in observational data. Instead you're see stuff like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#code to generate data that is loosely in the shape of a gaussian\n",
    "number_of_observations = 100\n",
    "noisy_gaussian = np.random.normal(size=number_of_observations) #generates 100 data points\n",
    "noisy_gaussian #displays data points below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an array of simulated data--i.e., the data has explicitly been sampled randomly from a gaussian distribution to \"simulate\" observational data. \n",
    "\n",
    "The \"array\" above is very similar to the python \"list\" discussed in the previous lab. (It's a array because this format allows calculations to be performed more quickly.) Numpy provides lots of handy functions to deal with data like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_value = np.min(noisy_gaussian)\n",
    "max_value = np.max(noisy_gaussian)\n",
    "average = np.average(noisy_gaussian)\n",
    "median = np.median(noisy_gaussian)\n",
    "standard_deviation = np.std(noisy_gaussian)\n",
    "\n",
    "#output values\n",
    "print(min_value)\n",
    "print(max_value)\n",
    "print(average) # Note that the average is approximately zero...\n",
    "print(median) # Note that this is *not* zero...\n",
    "print(standard_deviation) # for a gaussian curve, one standard deviation (std) away from mean is 34.1%; two std away from mean is 47.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this data such that the x-axis is the element of array and the y-axis is the value of the element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(noisy_gaussian, linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replot this same data, but instead arrange the elements from smallest to biggest by value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sorted(noisy_gaussian), linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also produce a histogram of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(noisy_gaussian, 30, normed=False, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even overlay the average and the median upon our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(noisy_gaussian, 30, normed=False, color='r')\n",
    "plt.axvline(np.average(noisy_gaussian), color='g', linewidth=2, linestyle='dashed')\n",
    "plt.axvline(np.median(noisy_gaussian), color='b', linewidth=2, linestyle='dashed')\n",
    "#note that the average and the median will likely be on top of each other...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even overlay our theoretical gaussian over our simulated data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values, bins, ignored = plt.hist(noisy_gaussian, 30, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, changing the number of bins that you use in a historgram will change how well the data appears to fit the a curve. To see this, we replot the above graph using 10 bins, 30 bins (as above), 60 bins, and 100 bins. What number of bins makes the data appear to fit the curve the best? Is this a \"reasonable\" number of bins? (This second question is dependent on the context of the data and your particular question.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for 10 bins\n",
    "values, bins, ignored = plt.hist(noisy_gaussian, 10, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "#for 30 bins\n",
    "values, bins, ignored = plt.hist(noisy_gaussian, 30, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "#for 60 bins\n",
    "values, bins, ignored = plt.hist(noisy_gaussian, 60, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "#for 100 bins\n",
    "values, bins, ignored = plt.hist(noisy_gaussian, 100, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, having more data can make it easier to see patterns. To show this, we reproduce the same 4 plots as above, but we do this for 1000 observations rather than 100. How does the binning of the histogram for this new data set affect the \"credibility\" of the claim that the data is a gaussian distribution?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate 1000 observations\n",
    "number_of_observations = 1000\n",
    "less_noisy_gaussian = np.random.normal(size=number_of_observations) #generates 100 data points\n",
    "\n",
    "#for 10 bins\n",
    "values, bins, ignored = plt.hist(less_noisy_gaussian, 10, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "#for 30 bins\n",
    "values, bins, ignored = plt.hist(less_noisy_gaussian, 30, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "#for 60 bins\n",
    "values, bins, ignored = plt.hist(less_noisy_gaussian, 60, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n",
    "\n",
    "#for 100 bins\n",
    "values, bins, ignored = plt.hist(less_noisy_gaussian, 100, normed=True, color='b')\n",
    "plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2)), linewidth=2, color='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can also easily generate a box plot (sometimes called a \"box and whiskers\" plot). Using our 100 observations from earlier, we get the following graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.boxplot(noisy_gaussian)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the red line denotes the median and the top and bottom of the box denote the first and third quartiles, the horizontal lines at the ends of the \"whiskers\" can mean different things (see [Wikipedia's box plot](https://en.wikipedia.org/wiki/Box_plot) for a partial list). The box itself is composed of an \"interquartile region,\" denoted IQR, defined as range between quartile 3 and quartile 1, and the default for matplotlib sets the \"whiskers\" equal to 1.5(IQR). Refer to the documentation for the [boxplot function](http://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html#matplotlib.axes.Axes.boxplot) to learn about the various whisker options.        \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data\n",
    "Now lets get some data to explore. Alain Desrosieres argues that the central tension in histories examining the roles of statistics in political discourse is that the statistical entities that statistics uses are both real and fabrications: real in that they must be taken as “uncontestable standards” of reference insofar as they serve as  compelling evidence for a particular claim; fabrications in that they are the result of “the provisional and fragile crowning of a series of conventions of equivalence between entities.”[1] The statistical entity of life expectancy, for instance, is real insofar as it serves as a proxy for the health of populations and individuals, and is used to justify disparities in health and life insurance pricing and coverage for different populations. Yet in calculating life expectancy, one quickly discovers not a single computational method, but hundreds—each with a different set of assumptions that yield different results. Deciding which life expectancy estimation to employ is tied up with what the measure will be used to do, and so involves political, ethical, and even moral decisions about who and what should be counted and excluded.[2] Tracing the historical transformation of a statistical entity from a contingent, context-sensitive description into a “universal” property provides insight into the political institutions that created it while also making legible the ways in which a statistical entity exerted a reciprocal pressure back upon the institutions and individuals that created them.[3] Exploring the political implications of statistical entities is further complicated by their historical tendency to be repurposed for use in new arguments. While life expectancy was first developed for assigning and categorizing individuals according to their likelihood of death while their life insurance policy was in force, this statistical category was subsequently put to more sinister purposes: namely, to “demonstrate” the existence of racial biological characteristics and then to serve as “evidence” that race was an appropriate category for screening immigrants.[4]\n",
    "\n",
    "<b>*Our immediate purpose here is to get some practice using Pandas to explore a data set.*</b>\n",
    "\n",
    "<hr/>\n",
    "<small>\n",
    "1. Alain Desrosieres, <em>The Politics of Large Numbers: A History of Statistical Reasoning</em>, Cambridge, MA: Harvard University Press, 1998, 324-325.\n",
    "\n",
    "2. Desrosieres, <em>The Politics of Large Numbers</em>, 325.\n",
    "\n",
    "3. Desrosieres, <em>The Politics of Large Numbers</em>, 324.\n",
    "\n",
    "4. Dan Bouk, <em>How Our Days Became Numbered: Risk and the Rise of the Statistical Individual</em>, Chicago, IL: University of Chicago Press, 187-188, 201-202.\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading data about life expectancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget data-ppf.github.io/labs/life.expectancy.countries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the name of the file will be listed above with the phrase \"'[file name]' saved\". Now check to make sure you have it by typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the name is changed when you download the file. We can rename this file as follows with the \"mv\" command. Replace [file name] below with the name of the file you downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! mv [file name] life.expectancy.countries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was obtained from here: http://apps.who.int/gho/data/node.main.688. As we did in lab 1, you can learn more about the assumptions of the data by examining the information provided there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the actual text of this csv file using the \"cat\" command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat life.expectancy.countries.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that csv stands for \"comma separated values,\" but, in practice, the values in a file can be separated by any character, including commas, semi-colons, and tabs. We can use the pandas library to explore this data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lifeexpectancy = pd.read_csv(\"life.expectancy.countries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily list this data by just calling the variable we assigned it to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lifeexpectancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas attempts to automatically format the data, but the data itself may not always be in a format that pandas can interpret correctly. In the case above, we can see that row 0 is really part of the data label and is not data itself. Note that the data gives the life expectancy for different countries (column 1) and different years (column 2), as well as for different \"sexes\" and ages \"at birth\" and \"age 60\". We can fix the header row (i.e., the row with column labels) in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rename header labels\n",
    "lifeexpectancy.columns = [\"country\", \"year\", \"life expectancy at birth (both sexes)\", \\\n",
    "                          \"life expectancy at birth (female)\", \"life expectancy at birth (male)\", \\\n",
    "                          \"life expectancy at age 60 (both sexes)\", \"life expectancy at age 60 (female)\", \\\n",
    "                          \"life expectancy at age 60 (male)\"]\n",
    "\n",
    "# remove row 0 with header info from lifeexpectancy dataframe\n",
    "lifeexpectancy.drop(lifeexpectancy.index[[0]]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out how many rows we have, we can use \"index\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lifeexpectancy.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 2940 rows. To pick out individual rows from this data, we can use what is called a slice in python. To wit, to output rows 2, 3, and 4, we write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lifeexpectancy[2:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the call \"describe\" to tell us information about these three rows by individual column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lifeexpectancy[2:5].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lifeexpectancy.groupby('country').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's say we want to plot the life expectancy for 6 countries from 2000 to 2014. How do we do it? Note that\n",
    "\n",
    "<code> lifeexpectancy[1:16][\"life expectancy at birth (both sexes)\"] </code>\n",
    "\n",
    "means takes rows 1-16 and the column \"life expectancy at birth (both sexes)\". If we don't specific rows, it will return all rows that meet criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get rows for the 5 countries we are examining\n",
    "Afghanistan = lifeexpectancy[lifeexpectancy['country']=='Afghanistan']\n",
    "Zambia = lifeexpectancy[lifeexpectancy['country']=='Zambia']\n",
    "United_States = lifeexpectancy[lifeexpectancy['country']=='United States of America']\n",
    "France = lifeexpectancy[lifeexpectancy['country']=='France']\n",
    "Iceland = lifeexpectancy[lifeexpectancy['country']=='Iceland']\n",
    "Sudan = lifeexpectancy[lifeexpectancy['country']=='Sudan']\n",
    "\n",
    "#get life expectancy at birth\n",
    "France_values = France[\"life expectancy at birth (both sexes)\"].values.astype(float)\n",
    "Afghanistan_values = Afghanistan[\"life expectancy at birth (both sexes)\"].values.astype(float)\n",
    "US_values = United_States[\"life expectancy at birth (both sexes)\"].values.astype(float)\n",
    "Zambia_values = Zambia[\"life expectancy at birth (both sexes)\"].values.astype(float)\n",
    "Iceland_values = Iceland[\"life expectancy at birth (both sexes)\"].values.astype(float)\n",
    "Sudan_values = Sudan[\"life expectancy at birth (both sexes)\"].values.astype(float)\n",
    "\n",
    "#years for x-axis\n",
    "years = Afghanistan[\"year\"].values.astype(float)\n",
    "\n",
    "#plot different countries\n",
    "plt.plot(years, Afghanistan_values, color = \"black\")\n",
    "plt.plot(years, France_values, color = \"green\")\n",
    "plt.plot(years, US_values, color = \"blue\")\n",
    "plt.plot(years, Zambia_values, color = \"red\")\n",
    "plt.plot(years, Iceland_values, color = \"purple\")\n",
    "plt.plot(years, Sudan_values, color = \"yellow\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Other useful pandas commands:\n",
    "\n",
    "# 1. How find a column that contains a key or keywords ---\n",
    "Example1 = lifeexpectancy[lifeexpectancy.country.isin(['A term you wish to find', 'a second term'])]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
